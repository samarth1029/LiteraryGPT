{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512 # features for every word\n",
    "d_model = 512 # output of attention model for every word\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(input_dim, 3 * d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = qkv_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'qkv distribution')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_val = torch.histc(qkv, bins = 200, min =-3, max = 3)\n",
    "x_val = np.arange(-1,1,0.01)*3\n",
    "plt.bar(x_val,y_val,align='center', color='forestgreen')\n",
    "plt.title('qkv distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3*head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0,2,1,3) # [batch_size, num_heads, seq_length, 3*head_dim]\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "d_k = q.size()[-1]\n",
    "scaled = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full(scaled.size(), float('-inf'))\n",
    "mask = torch.triu(mask, diagonal =1)\n",
    "mask[0][1] # mask for input to a single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3681,    -inf,    -inf,    -inf],\n",
       "        [ 0.0513, -0.4260,    -inf,    -inf],\n",
       "        [ 0.1146,  0.2172,  0.2196,    -inf],\n",
       "        [-0.3970,  0.7354,  0.6916,  0.3154]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled += mask\n",
    "scaled[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 4]),\n",
       " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6171, 0.3829, 0.0000, 0.0000],\n",
       "         [0.3107, 0.3442, 0.3451, 0.0000],\n",
       "         [0.1097, 0.3406, 0.3260, 0.2237]], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = F.softmax(scaled, dim = -1)\n",
    "attention.shape, attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = attention @ v\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scaled_dot_product(q,k,v,mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = (q @ k.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim =-1)\n",
    "    values = attention @ v\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 4]),\n",
       " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6171, 0.3829, 0.0000, 0.0000],\n",
       "         [0.3107, 0.3442, 0.3451, 0.0000],\n",
       "         [0.1097, 0.3406, 0.3260, 0.2237]], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, attention = scaled_dot_product(q,k,v,mask=mask)\n",
    "attention.shape, attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat the heads\n",
    "values = values.reshape(batch_size, sequence_length, num_heads*head_dim)\n",
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = nn.Linear(d_model, d_model)\n",
    "out = linear_layer(values)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put Everything Together!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B-> batch size; T-> seq length; head dim -> d_model/n_heads; n_embed = d_model = num_heads * head_dim\n",
    "- Get x -> token embedding + positional embedding -> [B, T, n_embed]\n",
    "- Get qkv -> linear layer(n_embed, 3 * d_model (attention model out size))\n",
    "- Pass x to qkv layer -> [B, T, 3 * d_model]\n",
    "- Break qkv to split it across all heads -> [B, T, num_heads, 3 * head dim]\n",
    "- Permute [B, T, num_heads, 3 * head dim] -> [B, num_heads, T, 3 * head dim]\n",
    "- Split q,k,v -> [B, num_heads, T, head_dim]\n",
    "- Scaled dot prod attention:\n",
    "    - values -> [B, num_heads, T, head_dim]\n",
    "    - attention -> [B, num_heads, T, T]\n",
    "- Concatenate heads in value -> [B, T, num_heads * head_dim]\n",
    "- Output linear layer -> [B, T, num_heads * head_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def scaled_dot_product(q,k,v,mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = (q @ k.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim =-1)\n",
    "    values = attention @ v\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3*d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f\"{x.size()= }\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"{qkv.size()= }\")\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3*self.head_dim)\n",
    "        print(f\"{qkv.size()= }\")\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        print(f\"{qkv.size()= }\")\n",
    "        q, k, v = qkv.chunk(3, dim =-1)\n",
    "        print(f\"{q.size()= } {k.size()= } {v.size()= }\")\n",
    "        values,attention = scaled_dot_product(q,k,v,mask)\n",
    "        print(f\"{values.size()= } {attention.size()= }\")\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"{values.size()= }\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"{out.size()= }\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size()= torch.Size([30, 5, 1024])\n",
      "qkv.size()= torch.Size([30, 5, 1536])\n",
      "qkv.size()= torch.Size([30, 5, 8, 192])\n",
      "qkv.size()= torch.Size([30, 8, 5, 192])\n",
      "q.size()= torch.Size([30, 8, 5, 64]) k.size()= torch.Size([30, 8, 5, 64]) v.size()= torch.Size([30, 8, 5, 64])\n",
      "values.size()= torch.Size([30, 8, 5, 64]) attention.size()= torch.Size([30, 8, 5, 5])\n",
      "values.size()= torch.Size([30, 5, 512])\n",
      "out.size()= torch.Size([30, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n",
    "model = MultiHeadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Dropout\n",
    "* In addition, we also apply dropout to reduce overfitting during training\n",
    "* Dropout can be applied in several places:\n",
    "    * for example, after computing the attention weights;\n",
    "    * or after multiplying the attention weights with the value vectors\n",
    "* Here, we will apply the dropout mask after computing the attention weights because it's more common\n",
    "* Furthermore, in this specific example, we use a dropout rate of 50%, which means randomly masking out half of the attention weights. (When we train the GPT model later, we will use a lower dropout rate, such as 0.1 or 0.2)\n",
    "* If we apply a dropout rate of 0.5 (50%), the non-dropped values will be scaled accordingly by a factor of 1/0.5 = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
    "example = torch.ones(6, 6) # create a matrix of ones\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionDropout(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "        \n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionCombinedQKVDropout(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.context_length = context_length\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "        # (b, num_tokens, 3 * embed_dim) --> (b, num_tokens, 3, num_heads, head_dim)\n",
    "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        # (b, num_tokens, 3, num_heads, head_dim) --> (3, b, num_heads, num_tokens, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        # (3, b, num_heads, num_tokens, head_dim) -> 3 times (b, num_head, num_tokens, head_dim)\n",
    "        queries, keys, values = qkv.unbind(0)\n",
    "        # (b, num_heads, num_tokens, head_dim) --> (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**-0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        # (b, num_heads, num_tokens, num_tokens) --> (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = attn_weights @ values\n",
    "        # (b, num_heads, num_tokens, head_dim) --> (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "        # (b, num_tokens, num_heads, head_dim) --> (b, num_tokens, embed_dim)\n",
    "        context_vec = context_vec.contiguous().view(batch_size, num_tokens, embed_dim)\n",
    "        context_vec = self.proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multi-head attention with PyTorch's scaled dot product attention and FlashAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below uses PyTorch's scaled_dot_product_attention function, which implements a memory-optimized version of self-attention called FlashAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHAPyTorchScaledDotProduct(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.context_length = context_length\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "        # (b, num_tokens, 3 * embed_dim) --> (b, num_tokens, 3, num_heads, head_dim)\n",
    "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        # (b, num_tokens, 3, num_heads, head_dim) --> (3, b, num_heads, num_tokens, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        # (3, b, num_heads, num_tokens, head_dim) -> 3 times (b, num_heads, num_tokens, head_dim)\n",
    "        queries, keys, values = qkv\n",
    "        use_dropout = 0. if not self.training else self.dropout\n",
    "        context_vec = nn.functional.scaled_dot_product_attention(\n",
    "            queries, keys, values, attn_mask=None, dropout_p=use_dropout, is_causal=True)\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(batch_size, num_tokens, self.d_out)\n",
    "        context_vec = self.proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch's torch.nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MHAPyTorchClass(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False, need_weights=True):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_out,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            bias=qkv_bias,\n",
    "            add_bias_kv=qkv_bias,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.need_weights = need_weights\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, _ = x.shape\n",
    "        # Ensure attn_mask is compatible with expected shape and `batch_first=True`\n",
    "        # No need to manually adjust for num_heads; ensure it's right for the sequence\n",
    "        if self.context_length >= num_tokens:\n",
    "            attn_mask = self.mask[:num_tokens, :num_tokens]\n",
    "        else:\n",
    "            attn_mask = self.mask[:self.context_length, :self.context_length]\n",
    "        # attn_mask broadcasting will handle batch_size dimension implicitly\n",
    "        attn_output, _ = self.multihead_attn(\n",
    "            x, x, x, attn_mask=attn_mask, need_weights=self.need_weights\n",
    "        )\n",
    "        output = self.proj(attn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch's torch.nn.MultiheadAttention with scaled_dot_product_attention\n",
    "* Set need_weights (default True) to False so that MultiheadAttention uses scaled_dot_product_attention \n",
    "* need_weights: If specified, returns `attn_output_weights` in addition to `attn_outputs`.\n",
    "           Set `need_weights=False` to use the optimized `scaled_dot_product_attention`\n",
    "           and achieve the best performance for MHA.\n",
    "           Default: `True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "batch_size = 8\n",
    "context_len = 1024\n",
    "embed_dim = 768\n",
    "num_heads = 8\n",
    "embeddings = torch.randn((batch_size, context_len, embed_dim), device=device)\n",
    "\n",
    "mha = MultiHeadAttention(embed_dim, embed_dim, num_heads)\n",
    "\n",
    "mha_dropout = MultiHeadAttentionDropout(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=8,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "mha_combined_qkv_dropout = MultiHeadAttentionCombinedQKVDropout(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=8,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "mha_pytorch_flash_attn = MHAPyTorchScaledDotProduct(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=8,\n",
    "    qkv_bias=False\n",
    ").to(device)\n",
    "\n",
    "mha_pytorch_class_default = MHAPyTorchClass(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=context_len,\n",
    "    dropout=0.0,\n",
    "    num_heads=8,\n",
    "    qkv_bias=False\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size()= torch.Size([8, 1024, 768])\n",
      "qkv.size()= torch.Size([8, 1024, 2304])\n",
      "qkv.size()= torch.Size([8, 1024, 8, 288])\n",
      "qkv.size()= torch.Size([8, 8, 1024, 288])\n",
      "q.size()= torch.Size([8, 8, 1024, 96]) k.size()= torch.Size([8, 8, 1024, 96]) v.size()= torch.Size([8, 8, 1024, 96])\n",
      "values.size()= torch.Size([8, 8, 1024, 96]) attention.size()= torch.Size([8, 8, 1024, 1024])\n",
      "values.size()= torch.Size([8, 1024, 768])\n",
      "out.size()= torch.Size([8, 1024, 768])\n",
      "x.size()= torch.Size([8, 1024, 768])\n",
      "qkv.size()= torch.Size([8, 1024, 2304])\n",
      "qkv.size()= torch.Size([8, 1024, 8, 288])\n",
      "qkv.size()= torch.Size([8, 8, 1024, 288])\n",
      "q.size()= torch.Size([8, 8, 1024, 96]) k.size()= torch.Size([8, 8, 1024, 96]) v.size()= torch.Size([8, 8, 1024, 96])\n",
      "values.size()= torch.Size([8, 8, 1024, 96]) attention.size()= torch.Size([8, 8, 1024, 1024])\n",
      "values.size()= torch.Size([8, 1024, 768])\n",
      "out.size()= torch.Size([8, 1024, 768])\n",
      "x.size()= torch.Size([8, 1024, 768])\n",
      "qkv.size()= torch.Size([8, 1024, 2304])\n",
      "qkv.size()= torch.Size([8, 1024, 8, 288])\n",
      "qkv.size()= torch.Size([8, 8, 1024, 288])\n",
      "q.size()= torch.Size([8, 8, 1024, 96]) k.size()= torch.Size([8, 8, 1024, 96]) v.size()= torch.Size([8, 8, 1024, 96])\n",
      "values.size()= torch.Size([8, 8, 1024, 96]) attention.size()= torch.Size([8, 8, 1024, 1024])\n",
      "values.size()= torch.Size([8, 1024, 768])\n",
      "out.size()= torch.Size([8, 1024, 768])\n",
      "x.size()= torch.Size([8, 1024, 768])\n",
      "qkv.size()= torch.Size([8, 1024, 2304])\n",
      "qkv.size()= torch.Size([8, 1024, 8, 288])\n",
      "qkv.size()= torch.Size([8, 8, 1024, 288])\n",
      "q.size()= torch.Size([8, 8, 1024, 96]) k.size()= torch.Size([8, 8, 1024, 96]) v.size()= torch.Size([8, 8, 1024, 96])\n",
      "values.size()= torch.Size([8, 8, 1024, 96]) attention.size()= torch.Size([8, 8, 1024, 1024])\n",
      "values.size()= torch.Size([8, 1024, 768])\n",
      "out.size()= torch.Size([8, 1024, 768])\n",
      "x.size()= torch.Size([8, 1024, 768])\n",
      "qkv.size()= torch.Size([8, 1024, 2304])\n",
      "qkv.size()= torch.Size([8, 1024, 8, 288])\n",
      "qkv.size()= torch.Size([8, 8, 1024, 288])\n",
      "q.size()= torch.Size([8, 8, 1024, 96]) k.size()= torch.Size([8, 8, 1024, 96]) v.size()= torch.Size([8, 8, 1024, 96])\n",
      "values.size()= torch.Size([8, 8, 1024, 96]) attention.size()= torch.Size([8, 8, 1024, 1024])\n",
      "values.size()= torch.Size([8, 1024, 768])\n",
      "out.size()= torch.Size([8, 1024, 768])\n",
      "x.size()= torch.Size([8, 1024, 768])\n",
      "qkv.size()= torch.Size([8, 1024, 2304])\n",
      "qkv.size()= torch.Size([8, 1024, 8, 288])\n",
      "qkv.size()= torch.Size([8, 8, 1024, 288])\n",
      "q.size()= torch.Size([8, 8, 1024, 96]) k.size()= torch.Size([8, 8, 1024, 96]) v.size()= torch.Size([8, 8, 1024, 96])\n",
      "values.size()= torch.Size([8, 8, 1024, 96]) attention.size()= torch.Size([8, 8, 1024, 1024])\n",
      "values.size()= torch.Size([8, 1024, 768])\n",
      "out.size()= torch.Size([8, 1024, 768])\n",
      "x.size()= torch.Size([8, 1024, 768])\n",
      "qkv.size()= torch.Size([8, 1024, 2304])\n",
      "qkv.size()= torch.Size([8, 1024, 8, 288])\n",
      "qkv.size()= torch.Size([8, 8, 1024, 288])\n",
      "q.size()= torch.Size([8, 8, 1024, 96]) k.size()= torch.Size([8, 8, 1024, 96]) v.size()= torch.Size([8, 8, 1024, 96])\n",
      "values.size()= torch.Size([8, 8, 1024, 96]) attention.size()= torch.Size([8, 8, 1024, 1024])\n",
      "values.size()= torch.Size([8, 1024, 768])\n",
      "out.size()= torch.Size([8, 1024, 768])\n",
      "x.size()= torch.Size([8, 1024, 768])\n",
      "qkv.size()= torch.Size([8, 1024, 2304])\n",
      "qkv.size()= torch.Size([8, 1024, 8, 288])\n",
      "qkv.size()= torch.Size([8, 8, 1024, 288])\n",
      "q.size()= torch.Size([8, 8, 1024, 96]) k.size()= torch.Size([8, 8, 1024, 96]) v.size()= torch.Size([8, 8, 1024, 96])\n",
      "values.size()= torch.Size([8, 8, 1024, 96]) attention.size()= torch.Size([8, 8, 1024, 1024])\n",
      "values.size()= torch.Size([8, 1024, 768])\n",
      "out.size()= torch.Size([8, 1024, 768])\n",
      "780 ms ± 50.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mha(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03 s ± 331 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mha_dropout(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.02 s ± 159 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mha_combined_qkv_dropout(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676 ms ± 118 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mha_pytorch_flash_attn(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802 ms ± 17.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mha_pytorch_class_default(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
